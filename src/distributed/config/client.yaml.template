# Configuration for scamp worker deployment
# You must apply service.yaml before creating a worker deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scamp-client
spec:
  # One replica for each GPU in the cluster
  replicas: 8
  selector:
    matchLabels:
      run: scamp-client
  template:
    metadata:
      labels:
        run: scamp-client
    spec:
      # Allow scheduling on gpu nodes on gke (automatically taints gpu nodes)
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      affinity:
        # Prefer to run on preemptible nodes on gke
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: cloud.google.com/gke-preemptible
                  operator: Exists
              weight: 100
        # Do not run on a node which is already running a server pod
        # SCAMP pods will automatically utilize all available CPUs or GPUs
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: run
                operator: In
                values: 
                - scamp-server
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: client
        # Public repository which contains the docker image for the SCAMP master branch
        image: zpzim/scamp:latest
        # Each container should use the maximum number of GPUs available on the node
        # However since this is not possible to do in a flexible way on kubernetes instead
        # We use one GPU per container and just create more pods
        resources:
          requests:
            memory: "2.5G"
            cpu: "250m"
          limits:
            nvidia.com/gpu: 1
        # Run the worker on startup
        command: ["/SCAMP/build/src/distributed/SCAMPclient"]
        ports:
        - containerPort: 30078
